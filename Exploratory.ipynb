{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365892c-269e-484d-8f16-30ea97021691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import gensim\n",
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d19ac77-e0b4-4ba9-8e42-de8c59807dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# pip install -U spacy\n",
    "# !python -m spacy download ru_core_news_lg\n",
    "nlp = spacy.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8d16d-a246-4b86-86c3-0ca486105363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('Excel_files/Full_Poem_Dataset_9-30_0.xlsx')\n",
    "print(len(df))\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df.drop_duplicates(subset=['Text'])\n",
    "print(len(df))\n",
    "records = df.to_dict('records')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536083c7-34ec-41f9-9ded-fd6e6c2768b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Before or after'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871f822-6daf-459d-a42e-1bb6fa395cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Before or after'] == 'Before']['Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91efc80-4ff3-4159-8bd9-24d83b8c40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Before or after'] == 'After']['Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa672d7b-a642-456c-9776-8a94534ac0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Print some key authors before and after\n",
    "# with open('херсонскийbefore.txt','w') as f:\n",
    "#     for b in df[df['Author'] == 'Борис Херсонский'][df['Before or after'] == 'Before'].sample(10).to_dict('records'):\n",
    "#         f.write('New Poem from '+ b['Source']+'\\n\\n'+b['Text']+'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05dd6bb-5b4a-41d5-b5a6-3f6535f22f5b",
   "metadata": {},
   "source": [
    "## Run Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1ee96-bb41-406d-918a-75378878bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "badIdxs = []\n",
    "for i, rec in enumerate(records):\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i}/{len(records)} parsed by Spacy.')\n",
    "    try:\n",
    "        lines = rec['Text'].split('\\n')\n",
    "        docLines = []\n",
    "        for line in lines:\n",
    "            docLines.append(nlp(line))\n",
    "        rec['docLines'] = docLines\n",
    "        # rec['doc'] = nlp(rec['Text'])\n",
    "    except:\n",
    "        badIdxs.append(i)\n",
    "for i in badIdxs:\n",
    "    records.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b15cf-8623-42da-a668-0c0933f96544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many tokens?\n",
    "bTotalTokens = 0\n",
    "aTotalTokens = 0\n",
    "for rec in records:\n",
    "    if rec['Before or after'] == 'Before':\n",
    "        bTotalTokens += len(rec['docLines'])\n",
    "    else:\n",
    "        aTotalTokens += len(rec['docLines'])\n",
    "bTotalTokens, aTotalTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e0c6a-df3a-4717-939b-29f5446b222a",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6da6e-9be1-4645-bcb8-e3b8e482d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeRec(recText):\n",
    "    if isinstance(recText, str):\n",
    "        return recText.strip()\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def skipLine(line, idx):\n",
    "    if len(line.strip()) == 0:\n",
    "        return False\n",
    "    \n",
    "    # throw hashtag line\n",
    "    if line.strip()[0] == '#':\n",
    "        return True\n",
    "    \n",
    "    # throw attribution line\n",
    "    for attr in ['из личного','личный блог','источник:','авторский блог']:\n",
    "        if attr in line.lower():\n",
    "            return True\n",
    "        \n",
    "    # dots at the beginning\n",
    "    matches = re.search(\"[\\*\\+\\^-_][*+^-_= ]+\", line.strip()) \n",
    "    if matches:\n",
    "        return True\n",
    "    \n",
    "    # is none of it alphanumeric\n",
    "    containsAlpha = False\n",
    "    for char in line:\n",
    "        if char.isalpha():\n",
    "            containsAlpha = True\n",
    "            break\n",
    "    if not containsAlpha:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def processRec(rec):\n",
    "    recText = initializeRec(rec['Text'])\n",
    "    cleanLines = []\n",
    "    if recText:\n",
    "        # decide which lines to keep\n",
    "        lines = recText.split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            if skipLine(line, i):\n",
    "                continue\n",
    "            cleanLines.append(line)\n",
    "    return '\\n'.join(cleanLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b83df-8d12-4a8f-b2aa-05bba0e80df3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing out the NLP capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b74eb-f875-45cd-bcd4-232e3b94dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER\n",
    "rec = random.choice(records)\n",
    "\n",
    "NERcounter = {'counts' : dict(), 'texts' : dict()}\n",
    "for rec in random.sample(records,10):\n",
    "    text = processRec(rec)\n",
    "    doc = nlp(text.replace('\\n', '; '))\n",
    "    for ent in doc.ents:\n",
    "        lemma = ent.lemma_\n",
    "        entType = ent.label_\n",
    "        NERcounter['counts'].setdefault(entType, dict())\n",
    "        NERcounter['texts'].setdefault(entType, dict())\n",
    "        NERcounter['counts'][entType].setdefault(lemma, 0)\n",
    "        NERcounter['texts'][entType].setdefault(lemma, [])\n",
    "        NERcounter['counts'][entType][lemma] += 1\n",
    "        NERcounter['texts'][entType][lemma].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abfa188-ea8f-465f-a48b-20435096eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NERcounter['counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e3167-c067-405b-94fd-ac358ce1b088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# proper nouns?\n",
    "rec = random.choice(records)\n",
    "\n",
    "PNcounter = {'counts' : dict(), 'texts' : dict()}\n",
    "for rec in random.sample(records,10):\n",
    "    text = processRec(rec)\n",
    "    doc = nlp(text.replace('\\n', '; ').lower())\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        lemma = token.lemma_\n",
    "        if pos == 'PROPN':\n",
    "            print(token, lemma, pos)\n",
    "        PNcounter['counts'].setdefault(lemma, 0)\n",
    "        PNcounter['texts'].setdefault(lemma, [])\n",
    "        PNcounter['counts'][lemma] += 1\n",
    "        PNcounter['texts'][lemma].append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d3e8a-f90b-4a62-b609-f74b5cd09df6",
   "metadata": {},
   "source": [
    "## Train new NER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03db9c-510f-4981-81fd-3e2a101e774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install nerus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5b10a-bee0-4e67-a74e-b5074b17f61b",
   "metadata": {},
   "source": [
    "### Load NERUS corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c6acf-9255-46af-947e-00cd9b440d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerus import load_nerus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0557a3-ca68-4258-9777-4869cf544446",
   "metadata": {},
   "outputs": [],
   "source": [
    "NERUS = 'nerus_lenta.conllu.gz'\n",
    "docs = load_nerus(NERUS)\n",
    "doc = next(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f565c-943e-4f29-acd6-376d530abc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "trainingSamples = []\n",
    "i = 0\n",
    "for doc in docs:\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i} docs parsed.')\n",
    "    for sent in doc.sents:\n",
    "        text = sent.text\n",
    "        entityDict = dict()\n",
    "        entityDict['entities'] = []\n",
    "        for span in sent.ner.spans:\n",
    "            entityDict['entities'].append((span.start, span.stop, span.type))\n",
    "\n",
    "        tokens = text.split(' ')\n",
    "        newTokens = []\n",
    "        for t in tokens:\n",
    "            if t.isupper() and len(t) > 1:\n",
    "                newCase = t\n",
    "            else:\n",
    "                newCase = t.lower()\n",
    "            newTokens.append(newCase)\n",
    "        newText = ' '.join(newTokens)\n",
    "\n",
    "        datum = (newText, entityDict)\n",
    "        trainingSamples.append(datum)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7535d5e-3058-4dae-96ce-739ccdfad6f4",
   "metadata": {},
   "source": [
    "### Train SpaCY pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e021b-9f77-4ad0-b85f-510c3a7ddbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to clean in such a way where NERs are detected\n",
    "ner=nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e86485-cdaf-4fda-befc-41e0f15dc440",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(trainingSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca2f0a-0e3e-4bc0-a6c9-e69b6967da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATA = [\n",
    "    # (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]})\n",
    "    # ]\n",
    "    \n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897ebe9-6bae-4e93-b1dd-fbe868ed9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b7562-bef8-402f-b243-f1726199f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(30):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = dict()\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87def0-5f1c-4a21-9fe0-d77eb16219fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I was driving a Alto\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f75b64-27a8-4de6-b1a9-45911b354670",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c99515-13af-42e4-9048-fa19a4cac49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the  model to directory\n",
    "output_dir = Path('/content/')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Load the saved model and predict\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
