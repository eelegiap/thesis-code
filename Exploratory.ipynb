{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1365892c-269e-484d-8f16-30ea97021691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import gensim\n",
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d19ac77-e0b4-4ba9-8e42-de8c59807dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# pip install -U spacy\n",
    "# !python -m spacy download ru_core_news_lg\n",
    "nlp = spacy.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b8d16d-a246-4b86-86c3-0ca486105363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2122\n",
      "2122\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Excel_files/Full_Poem_Dataset_9-22_1.xlsx')\n",
    "print(len(df))\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df.drop_duplicates(subset=['Text'])\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n",
    "records = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05dd6bb-5b4a-41d5-b5a6-3f6535f22f5b",
   "metadata": {},
   "source": [
    "## Run Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd1ee96-bb41-406d-918a-75378878bedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2122 parsed by Spacy.\n",
      "100/2122 parsed by Spacy.\n",
      "200/2122 parsed by Spacy.\n",
      "300/2122 parsed by Spacy.\n",
      "400/2122 parsed by Spacy.\n",
      "500/2122 parsed by Spacy.\n",
      "600/2122 parsed by Spacy.\n",
      "700/2122 parsed by Spacy.\n",
      "800/2122 parsed by Spacy.\n",
      "900/2122 parsed by Spacy.\n",
      "1000/2122 parsed by Spacy.\n",
      "1100/2122 parsed by Spacy.\n",
      "1200/2122 parsed by Spacy.\n",
      "1300/2122 parsed by Spacy.\n",
      "1400/2122 parsed by Spacy.\n",
      "1500/2122 parsed by Spacy.\n",
      "1600/2122 parsed by Spacy.\n",
      "1700/2122 parsed by Spacy.\n",
      "1800/2122 parsed by Spacy.\n",
      "1900/2122 parsed by Spacy.\n",
      "2000/2122 parsed by Spacy.\n",
      "2100/2122 parsed by Spacy.\n",
      "CPU times: user 7min 46s, sys: 27.3 s, total: 8min 13s\n",
      "Wall time: 10min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "badIdxs = []\n",
    "for i, rec in enumerate(records):\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i}/{len(records)} parsed by Spacy.')\n",
    "    try:\n",
    "        lines = rec['Text'].split('\\n')\n",
    "        docLines = []\n",
    "        for line in lines:\n",
    "            docLines.append(nlp(line))\n",
    "        rec['docLines'] = docLines\n",
    "        # rec['doc'] = nlp(rec['Text'])\n",
    "    except:\n",
    "        badIdxs.append(i)\n",
    "for i in badIdxs:\n",
    "    records.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd7b15cf-8623-42da-a668-0c0933f96544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32838, 37626)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many tokens?\n",
    "bTotalTokens = 0\n",
    "aTotalTokens = 0\n",
    "for rec in records:\n",
    "    if rec['Before or after'] == 'Before':\n",
    "        bTotalTokens += len(rec['docLines'])\n",
    "    else:\n",
    "        aTotalTokens += len(rec['docLines'])\n",
    "bTotalTokens, aTotalTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e0c6a-df3a-4717-939b-29f5446b222a",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59a6da6e-9be1-4645-bcb8-e3b8e482d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeRec(recText):\n",
    "    if isinstance(recText, str):\n",
    "        return recText.strip()\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def skipLine(line, idx):\n",
    "    if len(line.strip()) == 0:\n",
    "        return False\n",
    "    \n",
    "    # throw hashtag line\n",
    "    if line.strip()[0] == '#':\n",
    "        return True\n",
    "    \n",
    "    # throw attribution line\n",
    "    for attr in ['из личного','личный блог','источник:','авторский блог']:\n",
    "        if attr in line.lower():\n",
    "            return True\n",
    "        \n",
    "    # dots at the beginning\n",
    "    matches = re.search(\"[\\*\\+\\^-_][*+^-_= ]+\", line.strip()) \n",
    "    if matches:\n",
    "        return True\n",
    "    \n",
    "    # is none of it alphanumeric\n",
    "    containsAlpha = False\n",
    "    for char in line:\n",
    "        if char.isalpha():\n",
    "            containsAlpha = True\n",
    "            break\n",
    "    if not containsAlpha:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def processRec(rec):\n",
    "    recText = initializeRec(rec['Text'])\n",
    "    cleanLines = []\n",
    "    if recText:\n",
    "        # decide which lines to keep\n",
    "        lines = recText.split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            if skipLine(line, i):\n",
    "                continue\n",
    "            cleanLines.append(line)\n",
    "    return '\\n'.join(cleanLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b83df-8d12-4a8f-b2aa-05bba0e80df3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing out the NLP capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f42b74eb-f875-45cd-bcd4-232e3b94dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PER': {'хуёвый': 1}}\n",
      "\n",
      "молодая женщина в метро\n",
      "заслоняет от меня своего ребенка\n",
      "не смотри говорит не смотри\n",
      "приставляет ладонь словно шору\n",
      "не смотри на него мой козленочек\n",
      "вдруг ты сам таким станешь\n",
      "все люди с цветными волосами пидоры\n",
      "вам говорят что это неправда\n",
      "что есть маскулинные геи\n",
      "истинные защитники охуевшего в жопу отечества\n",
      "со стальными бицепсами\n",
      "не курящие марихуану\n",
      "но послушайте это неправда\n",
      "это продукт западной пропаганды\n",
      "призванной сбить вас с пути\n",
      "истинно говорю вам\n",
      "все с цветными волосами пидоры\n",
      "яркие девушки любят пизду\n",
      "синеволосые мальчики ебутся друг с другом\n",
      "мы не маргиналы и следим за здоровьем\n",
      "говорит переодетый в тетку мужик\n",
      "из локального транс-сообщества\n",
      "с хуевым как водится макияжем\n",
      "блондинистым париком тремя высшими\n",
      "образованиями\n",
      "но послушайте это неправда\n",
      "я читаю это обдолбанным\n",
      "на трезвую голову в жизни\n",
      "не додумаешься критиковать ваши скрепы\n",
      "лезть в ваши традиционные семьи\n",
      "тыкать хуем в гниль и тухлятину\n",
      "здесь вообще невозможно без трипа\n",
      "каждый вечер я обессиленный\n",
      "прихожу домой после этой войны\n",
      "повезло — не избитый свідомой фашней\n",
      "снимаю ботинки и распускаю волосы\n",
      "обнимаю себя\n",
      "падаю на диван перебирая перед сном\n",
      "словно фотокарточки в маленьком альбоме\n",
      "образы своих возлюбленных\n",
      "засекреченных суперагентов\n",
      "не выживших в квир-революции \n",
      " \n"
     ]
    }
   ],
   "source": [
    "rec = random.choice(records)\n",
    "text = processRec(rec)\n",
    "doc = nlp(text)\n",
    "\n",
    "NERcounter = dict()\n",
    "for ent in doc.ents:\n",
    "    lemma = ent.lemma_\n",
    "    entType = ent.label_\n",
    "    NERcounter.setdefault(entType, dict())\n",
    "    NERcounter[entType].setdefault(lemma, 0)\n",
    "    NERcounter[entType][lemma] += 1\n",
    "    \n",
    "print(NERcounter)\n",
    "print() \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b34ec6-e2dc-468c-8c5a-60b5669f9c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
