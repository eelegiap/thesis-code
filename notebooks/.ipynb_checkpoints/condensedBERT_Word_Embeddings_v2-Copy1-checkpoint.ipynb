{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBw5u4on4tk8"
   },
   "source": [
    "### Load Poetry Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PQLebu34szB",
    "outputId": "2c3356da-9374-48f7-bc37-97271976a2b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': '*\\nспят спокойно мёртвые россияне ворочаются во сне\\nподкладывают тёплые ладони под щёки\\nутром ведут в детский сад пока что живых детей\\nгде и на них найдутся\\nмаленькие гимнастёрки\\nигрушечные автоматы\\nпесня белеет ли в поле\\nмама мама что такое пороша\\nмама мама что за мальчик алёша\\nпочему он стоит\\nзначит ли что скоро и я\\nвстану над тёмным заросшим яром\\nне смогу снять каменные сапоги?\\nа если захочу закрыться рукой от солнца,\\nсмахнуть капельку пота выступившую над переносицей\\nнеужели поднять не смогу тяжелую руку?\\nмама мама что делать если воздух вдруг\\nразорвётся надвое, засияет?\\nспят спокойно мёртвые россияне\\n(прочитала в заметках к роману:\\nу всех живых — прозвища, а имена у мёртвых)\\nмама мамочка так страшно\\nстановится анечкой и алёшей\\nбольно лежать\\nокровавленной\\nраспростёртым',\n",
       " 'Author': 'Александра Шалашова',\n",
       " 'Before or after': 'After',\n",
       " 'Source': 'ROAR V2',\n",
       " 'Date posted': 'None'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "\n",
    "df = pd.read_excel('../Excel_files/Full_Poem_Dataset_10-23_3.xlsx')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "records = df.to_dict('records')\n",
    "random.choice(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RfUN_KolV-f",
    "outputId": "177ce662-8e78-4ad7-8b70-378071ebc4c7"
   },
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "lJEnBJ3gHTsQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pg0P9rFxJwwp",
    "outputId": "7b133608-bb30-4f8a-ce2f-20f18f9c8e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не стоит человек\n",
      "['[CLS]', 'не', 'стоит', 'человек', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = random.choice(random.choice(records)['Text'].split('\\n'))\n",
    "print(text)\n",
    "\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1z1SzuTrqx-7",
    "outputId": "9018432c-fd15-4603-9f65-433af389bbbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Чемпионов',\n",
       " '##uga',\n",
       " 'Рожер',\n",
       " 'именной',\n",
       " 'Milan',\n",
       " '##ящиеся',\n",
       " '##вам',\n",
       " '##ладзе',\n",
       " 'смонтированы',\n",
       " 'Горно']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list(tokenizer.vocab.keys()),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYjcYJuXoAQx",
    "outputId": "9d51a714-95ef-4c45-cd01-b11bce5f9ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "воды солёной вслушайся,\n",
      "[CLS]           101\n",
      "воды         12,240\n",
      "солё         46,590\n",
      "##ной         1,700\n",
      "всл          72,498\n",
      "##уша        29,991\n",
      "##й             860\n",
      "##ся          1,523\n",
      ",               128\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = random.choice(random.choice(records)['Text'].split('\\n'))\n",
    "print(text)\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_jEkVKxJMc0",
    "outputId": "7013263a-ac3a-4ce4-fa43-24ab26a6a570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 29761, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('следовательно')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"ru_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "E_t4cM6KLc98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3502 recs done.\n",
      "250/3502 recs done.\n",
      "500/3502 recs done.\n",
      "750/3502 recs done.\n",
      "1000/3502 recs done.\n",
      "1250/3502 recs done.\n",
      "1500/3502 recs done.\n",
      "1750/3502 recs done.\n",
      "2000/3502 recs done.\n",
      "2250/3502 recs done.\n",
      "2500/3502 recs done.\n",
      "2750/3502 recs done.\n",
      "3000/3502 recs done.\n",
      "3250/3502 recs done.\n",
      "3500/3502 recs done.\n"
     ]
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "data = []\n",
    "for i, rec in enumerate(records):\n",
    "    if i%250 == 0:\n",
    "        print(f'{i}/{len(records)} recs done.')\n",
    "    if not isinstance(rec['Text'],str):\n",
    "        continue\n",
    "    lines = rec['Text'].split('\\n')\n",
    "    for line in lines:\n",
    "        if len(line.split(' ')) < 5:\n",
    "            continue\n",
    "        lemmatized_line = ' '.join([l.lemma_ for l in nlp(line)])\n",
    "        doc = tokenizer.encode_plus(lemmatized_line)\n",
    "\n",
    "        data.append({\n",
    "            'doc' : doc,\n",
    "            'tokens' : tokenizer.convert_ids_to_tokens(doc['input_ids']),\n",
    "            'text' : line,\n",
    "            'lemmatized_text' : lemmatized_line,\n",
    "            'rec' : rec\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('records4RuBerta.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "87f6881696954d68800dbb82c525a601",
      "cbdccc6e24fa4dda8eeeb9da378f36b0",
      "d22add4eef6045cf95e4a72f7428ac9d",
      "4b3be7b64c9e4d79b6b0ce55daade852",
      "4849073628e2430a952fa97d1ec101e6",
      "a5175108f4e24b68a0c37e6e495df625",
      "03a5280fd1824ad4b9ee0ae1ef93e96a",
      "af9fc23c333b45d3ba6e0987ba608814",
      "407824ffad4749758b27200149e731f0",
      "43eda06351504eff8deb8204cece4119",
      "6b29f70530354c8a813b99dcbb0d721d"
     ]
    },
    "collapsed": true,
    "id": "Mq2PKplWfbFv",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "12b77a5b-b75f-461f-cba8-20d341dd64e2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('DeepPavlov/rubert-base-cased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4Qa5KkkM2Aq"
   },
   "source": [
    "Next, let's evaluate BERT on our example text, and fetch the hidden states of the network!\n",
    "\n",
    "*Side note: `torch.no_grad` tells PyTorch not to construct the compute graph during this forward pass (since we won't be running backprop here)--this just reduces memory consumption and speeds things up a little.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = [d for d in data if 'война' in d['lemmatized_text']]\n",
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "nN0QTZwiMzeq",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/744 recs done.\n",
      "250/744 recs done.\n",
      "500/744 recs done.\n",
      "CPU times: user 1min 19s, sys: 1.96 s, total: 1min 21s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the text through BERT, and collect all of the hidden states produced from all 12 layers. \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, d in enumerate(sample):\n",
    "        if i%250 == 0:\n",
    "            print(f'{i}/{len(sample)} recs done.')\n",
    "\n",
    "        tokens_tensor = torch.tensor([d['doc']['input_ids']])\n",
    "        segments_tensor = torch.tensor([d['doc']['attention_mask']])\n",
    "        \n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "        d['hidden_states'] = hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046\n"
     ]
    }
   ],
   "source": [
    "vecs = []\n",
    "for d in sample:\n",
    "    hidden_states = d['hidden_states']\n",
    "    last_n_layers = 2\n",
    "    # create a new dimension in the tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # print(token_embeddings.size())\n",
    "\n",
    "    # Remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # print(token_embeddings.size())\n",
    "\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    # print(token_embeddings.size())\n",
    "\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-2:-1], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    # print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "\n",
    "    for i, token_str in enumerate(d['tokens']):\n",
    "        # print(i, token_str)\n",
    "        if token_str == 'война':\n",
    "            vec = token_vecs_sum[i]\n",
    "            vecs.append({\n",
    "                'vec' : vec,\n",
    "                'text' : d['text']\n",
    "            })\n",
    "print(len(vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "simtexts = []\n",
    "for tup in itertools.combinations(vecs, 2):\n",
    "    v1, v2 = tup\n",
    "    if v1['text'] != v2['text']:\n",
    "        vec1 = v1['vec']\n",
    "        vec2 = v2['vec']\n",
    "        sim = 1 - cosine(vec1, vec2)\n",
    "        simtexts.append({\n",
    "            'sim' : sim,\n",
    "            'text1' : v1['text'],\n",
    "            'text2' : v2['text']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedtexts = sorted(simtexts, key=lambda x: x['sim'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sim': 1,\n",
       " 'text1': 'ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА',\n",
       " 'text2': 'ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА '}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedtexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sim': 0.14227454364299774,\n",
       " 'text1': 'ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ВОЙНА ',\n",
       " 'text2': 'Война обнажает запах открытых ран,'}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedtexts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in vecs:\n",
    "    v['vec'] = v['vec'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('война-10-23.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vecs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.54\n",
      "Vector similarity for *different* meanings:  0.62\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_cat[1], token_vecs_cat[3])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_cat[3], token_vecs_cat[5])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('First 5 vector values for each instance of \"игра-\".')\n",
    "# print('')\n",
    "# print(\"флейте   \", str(token_vecs_sum[2][:5]))\n",
    "# print(\"футбол  \", str(token_vecs_sum[11][:5]))\n",
    "# print(\"тубе   \", str(token_vecs_sum[20][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca2TCQ_G7SM3"
   },
   "source": [
    "We can see that the values differ, but let's calculate the cosine similarity between the vectors to make a more precise comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n",
    "# `hidden_states` is a Python list.\n",
    "print('      Type of hidden_states: ', type(hidden_states))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7jroXfKspe_"
   },
   "source": [
    "This looks pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orjhWUJgmxo5",
    "tags": []
   },
   "source": [
    "## 3.5. Pooling Strategy & Layer Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1CI97kNn8dD"
   },
   "source": [
    "Below are a couple additional resources for exploring this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3D5qnRNmq5_"
   },
   "source": [
    "**BERT Authors**\n",
    "\n",
    "The BERT authors tested word-embedding strategies by feeding different vector combinations as input features to a BiLSTM used on a named entity recognition task and observing the resulting F1 scores.\n",
    "\n",
    "(Image from [Jay Allamar](http://jalammar.github.io/illustrated-bert/)'s blog)\n",
    "\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)\n",
    "\n",
    "While concatenation of the last four layers produced the best results on this specific task, many of the other methods come in a close second and in general it is advisable to test different versions for your specific application: results may vary.\n",
    "\n",
    "This is partially demonstrated by noting that the different layers of BERT encode very different kinds of information, so the appropriate pooling strategy will change depending on the application because different layers encode different kinds of information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7_CVgejm5pr"
   },
   "source": [
    "**Han Xiao's BERT-as-service**\n",
    "\n",
    "Han Xiao created an open-source project named [bert-as-service](https://github.com/hanxiao/bert-as-service) on GitHub which is intended to create word embeddings for your text using BERT. Han experimented with different approaches to combining these embeddings, and shared some conclusions and rationale on the [FAQ page](https://github.com/hanxiao/bert-as-service#speech_balloon-faq) of the project. \n",
    "\n",
    "`bert-as-service`, by default, uses the outputs from the **second-to-last layer** of the model. \n",
    "\n",
    "I would summarize Han's perspective by the following:\n",
    "\n",
    "1. The embeddings start out in the first layer as having no contextual information (i.e., the meaning of the initial 'bank' embedding isn't specific to river bank or financial bank).\n",
    "2. As the embeddings move deeper into the network, they pick up more and more contextual information with each layer.\n",
    "3. As you approach the final layer, however, you start picking up information that is specific to BERT's pre-training tasks (the \"Masked Language Model\" (MLM) and \"Next Sentence Prediction\" (NSP)). \n",
    "    * What we want is embeddings that encode the word meaning well... \n",
    "    * BERT is motivated to do this, but it is also motivated to encode anything else that would help it determine what a missing word is (MLM), or whether the second sentence came after the first (NSP). \n",
    "4. The second-to-last layer is what Han settled on as a reasonable sweet-spot.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0unZ2xh4QDap"
   },
   "source": [
    "## 4.4. Implementations\n",
    "\n",
    "You can use the code in this notebook as the foundation of your own application to extract BERT features from text. However, official [tensorflow](https://github.com/google-research/bert/blob/master/extract_features.py) and well-regarded [pytorch](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/extract_features.py) implementations already exist that do this for you.  Additionally, [bert-as-a-service](https://github.com/hanxiao/bert-as-service) is an excellent tool designed specifically for running this task with high performance, and is the one I would recommend for production applications. The author has taken great care in the tool's implementation and provides excellent documentation (some of which was used to help create this tutorial) to help users understand the more nuanced details the user faces, like resource management and pooling strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhbZxbKRxMvM",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cite\n",
    "Chris McCormick and Nick Ryan. (2019, May 14). *BERT Word Embeddings Tutorial*. Retrieved from http://www.mccormickml.com\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03a5280fd1824ad4b9ee0ae1ef93e96a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "407824ffad4749758b27200149e731f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "43eda06351504eff8deb8204cece4119": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4849073628e2430a952fa97d1ec101e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b3be7b64c9e4d79b6b0ce55daade852": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43eda06351504eff8deb8204cece4119",
      "placeholder": "​",
      "style": "IPY_MODEL_6b29f70530354c8a813b99dcbb0d721d",
      "value": " 714M/714M [00:16&lt;00:00, 53.1MB/s]"
     }
    },
    "6b29f70530354c8a813b99dcbb0d721d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87f6881696954d68800dbb82c525a601": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cbdccc6e24fa4dda8eeeb9da378f36b0",
       "IPY_MODEL_d22add4eef6045cf95e4a72f7428ac9d",
       "IPY_MODEL_4b3be7b64c9e4d79b6b0ce55daade852"
      ],
      "layout": "IPY_MODEL_4849073628e2430a952fa97d1ec101e6"
     }
    },
    "a5175108f4e24b68a0c37e6e495df625": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af9fc23c333b45d3ba6e0987ba608814": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbdccc6e24fa4dda8eeeb9da378f36b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5175108f4e24b68a0c37e6e495df625",
      "placeholder": "​",
      "style": "IPY_MODEL_03a5280fd1824ad4b9ee0ae1ef93e96a",
      "value": "Downloading: 100%"
     }
    },
    "d22add4eef6045cf95e4a72f7428ac9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af9fc23c333b45d3ba6e0987ba608814",
      "max": 714314041,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_407824ffad4749758b27200149e731f0",
      "value": 714314041
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
