{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c1f973-8e3c-42fb-beb2-8eae75e46db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9e97f2-a55b-4a46-98ba-f6dcdfd86baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': 'А как выйдет младенец биясь и крича\\nтак сосед вызывает мента и врача\\nи склонившись над яслями оба \\nдержат тайный совет с ослецом и тельцом\\nоттесняя подальше Марию с отцом \\nне мелькайте мамаша я тут говорю с понятыми. \\n \\nИ пока ему мент указует козу \\nврач глядит ему в черные эти глаза\\nеврейчонка зайчонка сучонка\\nи жалеет соседа бессонного пса\\nи не знает какого тут сунуть упса \\nсамому бы принять и поспать до второго захода. \\n \\nИ пока ему врач отмеряет на вес, \\nмент успел обойти весь овин и завис\\nибо тоже не зверь но зверяче\\nозирает пеленки трусца пальтеца\\nпочерневших с лица ослеца и тельца \\nтут бы просто свалить но ведь сука опять дозвонится.\\n \\nИ пока ему каплют в развешенный рот\\nон орет арамейским орет и орет \\nбатя мнется Марию колбасит \\nи в запаре ослец не сказал ли чего\\nи как может телец утешает его \\nну сказал выпивают немножко живые же люди. \\n \\nА орет и орет он ногами суча\\nпро тельца ослеца и мента и врача\\nи соседа конечно соседа\\nнадрываясь и членик себе теребя\\nон орет \"я, зараза, достану тебя - \\nно не так, как ты мог бы себе скудоумно представить.\"\\n \\nИ соседа внезапно смертельная мгла\\nнакрывает и душит в кухонном углу\\nи становится страшным окошко\\nпотому как бы вроде и грязный расклад \\nпонаехали сквотят младенец орет\\nно вот прямо мента и врача психанул я чего-то. \\n \\nИ крадется ко хлеви во страхе ночном\\nи руками махает под хлипким окном\\nмол, не надо, валите, ребята,\\nтолько мент уже пишет чего-то в блокнот, \\nтолько врач уже пишет чего-то в блокнот,\\nзавтра в шесть на вокзале скажите спасибо не звали опеку. \\n \\nИ бежит он домой прошибая кусты\\nи впускает на кухню врача и мента\\nи мычит а они отвечают\\nи хотят говорить о себе и других\\nа выходит об этом засранце в яслях\\nпотому что собрались во имя его он сопит и смеется.\\n \\nА в хлеву тишина. В небесах тишина. \\nПять пятнадцать. Из хлева выходит она. \\nСпит младенец. Нагружен Иосиф.\\nИ у желтой звезды на его рукаве\\nочертания Иродовой головы\\nс бородой и в короне но это конечно случайно. \\n \\nА телятя ослятю бодает под дых\\nчто им будет за то что укрыли таких\\nа телятя мумучет и плачет \\nмент же думает молча слипая глаза\\nчто-то братьям в коринф я давно не писал\\nврач же думает молча слипая глаза \\nа хорошая вещь этот самый упса\\nкак не выпить-то весь пузырек расскажи мне парнишка.',\n",
       " 'Author': 'Линор Горалик',\n",
       " 'Before or after': 'Before',\n",
       " 'Source': 'essentialpoetry',\n",
       " 'Date posted': datetime.datetime(2020, 11, 7, 0, 0),\n",
       " 'UniqueIndex': 2539}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../Excel_files/Full_Poem_Dataset_12-17.xlsx')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "records = df.to_dict('records')\n",
    "random.choice(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b9229-bc58-4f3e-a0ae-28a37789f49f",
   "metadata": {},
   "source": [
    "## Word Co-occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b227b56-aeef-4944-aff5-1235b96ad4ec",
   "metadata": {},
   "source": [
    "### Initialize Spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371f9312-6a74-41ee-aa5e-21f121b3be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2dcde1-6e44-45a8-9608-4fab8aeafbcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download uk_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52702e34-a4ff-4035-a28a-2d0c84216d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download ru_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3b5557-9ce4-4537-8499-a02eda62f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_lg\", disable=['attribute_ruler','ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942516f5-328e-43c1-bbe7-7569aaafc87f",
   "metadata": {},
   "source": [
    "### Generate Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1efa21f-a33d-4adb-9c00-b8f280d09221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3222/3222 [02:54<00:00, 18.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for rec in tqdm(records):\n",
    "    recTxt = rec['Text']\n",
    "    # if not isinstance(recTxt,str):\n",
    "    #     recTxt = str(recTxt)\n",
    "    doc = nlp(recTxt)\n",
    "    rec['spacydoc'] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b179b-158c-4d4e-a97b-897dee35c264",
   "metadata": {},
   "source": [
    "### Run cooc parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b78a34-0444-4d3d-93a0-9a06e49006b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "140544ad-c44a-4e92-81e1-0da4761dfc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.ru import stop_words\n",
    "ru_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.en import stop_words\n",
    "en_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.uk import stop_words\n",
    "uk_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.es import stop_words\n",
    "es_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.pl import stop_words\n",
    "pl_stops = stop_words.STOP_WORDS\n",
    "\n",
    "stopwords = list(ru_stops)+list(uk_stops)\n",
    "foreign_stops = list(en_stops)+list(es_stops)+list(pl_stops)\n",
    "short_stops = [s for s in stopwords if len(s) < 3]\n",
    "short_ru_stops = [s for s in ru_stops if len(s) < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e983f2f6-8b37-4ea4-8973-2a4965df02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isLemma(txt):\n",
    "    for p in string.punctuation+'—'+'–':\n",
    "        if p in txt:\n",
    "            return False\n",
    "    if txt.isspace():\n",
    "        return False\n",
    "    if txt in foreign_stops:\n",
    "        return False\n",
    "    # if txt in short_stops:\n",
    "    #     return False\n",
    "    if txt in stopwords:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8995008-9391-4d6c-96cd-28eee8c64428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isLemma('говорить')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202d39e-5e2f-44de-95d0-f0e0811d1b62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Before and after separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed635a2f-dc8d-4b57-b0d5-2fdea3c2ca6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding sufficient nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3222/3222 [00:02<00:00, 1281.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3222/3222 [00:11<00:00, 291.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 33, 27, 26, 25, 24, 23, 23, 22, 22]\n",
      "162859\n",
      "wrote the jsons!\n"
     ]
    }
   ],
   "source": [
    "for period in ['Before','After'][1:]:\n",
    "    \n",
    "    lens=set()\n",
    "    lenCts = dict()\n",
    "\n",
    "    networkJson = dict()\n",
    "    networkJson['nodes'] = []\n",
    "    networkJson['links'] = []\n",
    "\n",
    "    curatedNodes = set()\n",
    "    linkCounter = dict()\n",
    "    lemmaCounter = dict()\n",
    "\n",
    "    print('finding sufficient nodes...')\n",
    "\n",
    "    for rec in tqdm(records):\n",
    "        # if rec['Before or after'] != period:\n",
    "            # continue\n",
    "            \n",
    "        recTxt = rec['Text']\n",
    "\n",
    "        doc = rec['spacydoc']\n",
    "        for t1 in doc:\n",
    "            l1 = t1.lemma_.lower()\n",
    "            lemmaCounter.setdefault(l1, 0)\n",
    "            lemmaCounter[l1] += 1\n",
    "\n",
    "    # find those nodes which occur enough times\n",
    "    sufficientNodes = set()\n",
    "    for lemma in lemmaCounter:\n",
    "        if lemmaCounter[lemma] >= 5 and isLemma(lemma):\n",
    "            sufficientNodes.add(lemma.lower())\n",
    "\n",
    "\n",
    "    label2lines = dict()\n",
    "    # label2recs = dict()\n",
    "    label2authors = dict()\n",
    "\n",
    "    print('parsing links...')\n",
    "    for rec in tqdm(records):\n",
    "        \n",
    "        if rec['Before or after'] != period:\n",
    "            continue\n",
    "        \n",
    "        doc = rec['spacydoc']\n",
    "        newdoc = [t for t in doc if (t.text.isalpha() or t.text == '\\n')]\n",
    "        doclen = len(newdoc)\n",
    "        # # line jumping\n",
    "        allLines = []\n",
    "        newLine = []\n",
    "        for i, t in enumerate(newdoc):\n",
    "            if t.text == '\\n':\n",
    "                allLines.append(newLine)\n",
    "                newLine = []\n",
    "            elif i == doclen-1:\n",
    "                allLines.append(newLine)\n",
    "            else:\n",
    "                newLine.append(t)\n",
    "        numLines = len(allLines)\n",
    "        windowlength = 2\n",
    "\n",
    "        labelFoundFromLine = dict()\n",
    "        for i1 in range(numLines-windowlength):\n",
    "            # all tokens in the desired range\n",
    "            tokensInWindow = []\n",
    "            excerptLines = []\n",
    "            for line in allLines[i1:i1+windowlength]:\n",
    "                excerptLines.append(' '.join([t.text for t in line]))\n",
    "                for token in line:\n",
    "                    tokensInWindow.append(token)\n",
    "            tiw = len(tokensInWindow)\n",
    "            lens.add(tiw)\n",
    "            lenCts.setdefault(tiw,0)\n",
    "            lenCts[tiw] += 1\n",
    "            if tiw > 50:\n",
    "                continue\n",
    "\n",
    "            for ic, (t1, t2) in enumerate(itertools.combinations(tokensInWindow,2)):\n",
    "                l1 = t1.lemma_.lower()\n",
    "                l2 = t2.lemma_.lower()\n",
    "                if l1 != l2 and l1 in sufficientNodes and l2 in sufficientNodes:\n",
    "                    label = 'AND'.join(sorted([l1, l2]))\n",
    "\n",
    "                    ### label already found in line?\n",
    "                    repeatedInstance = False\n",
    "                    for line in excerptLines:\n",
    "                        labelFoundFromLine.setdefault((label, line), False)\n",
    "                        if labelFoundFromLine[(label, line)]:\n",
    "                            repeatedInstance = True\n",
    "                        else:\n",
    "                            labelFoundFromLine[(label, line)] = True\n",
    "                    if repeatedInstance:\n",
    "                        continue\n",
    "\n",
    "                    label2authors.setdefault(label, {'Before' : set(), 'After' : set()})\n",
    "                    label2authors[label][period].add(rec['Author'])\n",
    "\n",
    "#                     label2recs.setdefault(label, [])\n",
    "#                     if rec not in label2recs[label]:\n",
    "#                         label2recs[label].append(rec)\n",
    "\n",
    "                    excerpt = '\\n'.join(excerptLines)\n",
    "                    label2lines.setdefault(label, [])\n",
    "                    if excerpt not in [e['excerpt'] for e in label2lines[label]]:\n",
    "                        # increase link Ct\n",
    "                        linkCounter.setdefault(label, 0)\n",
    "                        linkCounter[label] += 1\n",
    "                        period = rec['Before or after']\n",
    "                        if pd.isna(period):\n",
    "                            period = ''\n",
    "                        author = rec['Author']\n",
    "                        if pd.isna(author):\n",
    "                            author = 'Unknown'\n",
    "                        label2lines[label].append({\n",
    "                            'excerpt' : excerpt,\n",
    "                            'author' : author,\n",
    "                            'period' : period,\n",
    "                            'uniqueIndex' : rec['UniqueIndex']\n",
    "                        })\n",
    "\n",
    "    node2id = dict()\n",
    "    for i, node in enumerate(sufficientNodes):\n",
    "        networkJson['nodes'].append({\n",
    "            'id' : node,\n",
    "            'totalinstances' : lemmaCounter[node]\n",
    "        })\n",
    "        node2id[node] = i\n",
    "\n",
    "    for label in linkCounter:\n",
    "        source = label.split('AND')[0]\n",
    "        target = label.split('AND')[1]\n",
    "        if lemmaCounter[source] > 9 and lemmaCounter[target] > 9:\n",
    "            networkJson['links'].append({\n",
    "                'source' : node2id[source],\n",
    "                'target' : node2id[target],\n",
    "                'sourceLemma' : source,\n",
    "                'targetLemma' : target,\n",
    "                'linkCt' : linkCounter[label],\n",
    "                'authorCt' : list(label2authors[label])\n",
    "            })\n",
    "    print(sorted([l['linkCt'] for l in networkJson['links']],reverse=True)[:10])\n",
    "    print(len(networkJson['links']))\n",
    "    with open(f'../wordnet/data/{period}-wordnet2lines_allstops_2-21.json', 'w') as json_file:\n",
    "        json.dump(networkJson, json_file, ensure_ascii = False, indent=4)\n",
    "    with open(f'../wordnet/data/{period}-label2lines_2-21.json', 'w') as json_file:\n",
    "        json.dump(label2lines, json_file, ensure_ascii=False, indent=4)\n",
    "    print('wrote the jsons!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7eea5d-5a9a-4791-9cbd-1f0341b0fa4f",
   "metadata": {},
   "source": [
    "## Before and After Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfc7f9c6-1b6a-446f-9e0d-683086b20ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 4502, 'target': 1881, 'sourceLemma': 'менять', 'targetLemma': 'форма', 'linkCt': 2, 'authorCt': ['Виктор Плавский', 'Василий Бородин']}\n",
      "\n",
      "Видимо когда что то привычное\n",
      "меняет свою форму\n",
      "\n",
      "неразрывного одного объема\n",
      "чуть меняющего форму когда в него бьют\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lens=set()\n",
    "lenCts = dict()\n",
    "\n",
    "networkJson = dict()\n",
    "networkJson['nodes'] = []\n",
    "networkJson['links'] = []\n",
    "\n",
    "curatedNodes = set()\n",
    "linkCounter = dict()\n",
    "lemmaCounter = dict()\n",
    "\n",
    "print('finding sufficient nodes...')\n",
    "\n",
    "for rec in tqdm(records):\n",
    "    # if rec['Before or after'] != period:\n",
    "        # continue\n",
    "\n",
    "    recTxt = rec['Text']\n",
    "\n",
    "    doc = rec['spacydoc']\n",
    "    for t1 in doc:\n",
    "        l1 = t1.lemma_.lower()\n",
    "        lemmaCounter.setdefault(l1, 0)\n",
    "        lemmaCounter[l1] += 1\n",
    "\n",
    "# find those nodes which occur enough times\n",
    "sufficientNodes = set()\n",
    "for lemma in lemmaCounter:\n",
    "    if lemmaCounter[lemma] >= 5 and isLemma(lemma):\n",
    "        sufficientNodes.add(lemma.lower())\n",
    "\n",
    "\n",
    "label2lines = dict()\n",
    "label2recs = dict()\n",
    "label2authors = dict()\n",
    "\n",
    "print('parsing links...')\n",
    "for rec in tqdm(records):\n",
    "\n",
    "    period = rec['Before or after']\n",
    "\n",
    "    doc = rec['spacydoc']\n",
    "    newdoc = [t for t in doc if (t.text.isalpha() or t.text == '\\n')]\n",
    "    doclen = len(newdoc)\n",
    "    # # line jumping\n",
    "    allLines = []\n",
    "    newLine = []\n",
    "    for i, t in enumerate(newdoc):\n",
    "        if t.text == '\\n':\n",
    "            allLines.append(newLine)\n",
    "            newLine = []\n",
    "        elif i == doclen-1:\n",
    "            allLines.append(newLine)\n",
    "        else:\n",
    "            newLine.append(t)\n",
    "    numLines = len(allLines)\n",
    "    windowlength = 2\n",
    "\n",
    "    labelFoundFromLine = dict()\n",
    "    for i1 in range(numLines-windowlength):\n",
    "        # all tokens in the desired range\n",
    "        tokensInWindow = []\n",
    "        excerptLines = []\n",
    "        for line in allLines[i1:i1+windowlength]:\n",
    "            excerptLines.append(' '.join([t.text for t in line]))\n",
    "            for token in line:\n",
    "                tokensInWindow.append(token)\n",
    "        tiw = len(tokensInWindow)\n",
    "        lens.add(tiw)\n",
    "        lenCts.setdefault(tiw,0)\n",
    "        lenCts[tiw] += 1\n",
    "        if tiw > 50:\n",
    "            continue\n",
    "\n",
    "        for ic, (t1, t2) in enumerate(itertools.combinations(tokensInWindow,2)):\n",
    "            l1 = t1.lemma_.lower()\n",
    "            l2 = t2.lemma_.lower()\n",
    "            if l1 != l2 and l1 in sufficientNodes and l2 in sufficientNodes:\n",
    "                label = 'AND'.join(sorted([l1, l2]))\n",
    "\n",
    "                ### label already found in line?\n",
    "                repeatedInstance = False\n",
    "                for line in excerptLines:\n",
    "                    labelFoundFromLine.setdefault((label, line), False)\n",
    "                    if labelFoundFromLine[(label, line)]:\n",
    "                        repeatedInstance = True\n",
    "                    else:\n",
    "                        labelFoundFromLine[(label, line)] = True\n",
    "                if repeatedInstance:\n",
    "                    continue\n",
    "\n",
    "                label2authors.setdefault(label, set())\n",
    "                label2authors[label].add(rec['Author'])\n",
    "\n",
    "                label2recs.setdefault(label, [])\n",
    "                if rec not in label2recs[label]:\n",
    "                    label2recs[label].append(rec)\n",
    "\n",
    "                excerpt = '\\n'.join(excerptLines)\n",
    "                label2lines.setdefault(label, [])\n",
    "                if excerpt not in [e['excerpt'] for e in label2lines[label]]:\n",
    "                    # increase link Ct\n",
    "                    linkCounter.setdefault(label, 0)\n",
    "                    linkCounter[label] += 1\n",
    "                    period = rec['Before or after']\n",
    "                    if pd.isna(period):\n",
    "                        period = ''\n",
    "                    author = rec['Author']\n",
    "                    if pd.isna(author):\n",
    "                        author = 'Unknown'\n",
    "                    label2lines[label].append({\n",
    "                        'excerpt' : excerpt,\n",
    "                        'author' : author,\n",
    "                        'period' : period,\n",
    "                        'uniqueIndex' : rec['UniqueIndex']\n",
    "                    })\n",
    "\n",
    "node2id = dict()\n",
    "for i, node in enumerate(sufficientNodes):\n",
    "    networkJson['nodes'].append({\n",
    "        'id' : node,\n",
    "        'totalinstances' : lemmaCounter[node]\n",
    "    })\n",
    "    node2id[node] = i\n",
    "\n",
    "for label in linkCounter:\n",
    "    source = label.split('AND')[0]\n",
    "    target = label.split('AND')[1]\n",
    "    if lemmaCounter[source] > 9 and lemmaCounter[target] > 9:\n",
    "        networkJson['links'].append({\n",
    "            'source' : node2id[source],\n",
    "            'target' : node2id[target],\n",
    "            'sourceLemma' : source,\n",
    "            'targetLemma' : target,\n",
    "            'linkCt' : linkCounter[label],\n",
    "            'authorCt' : list(label2authors[label])\n",
    "        })\n",
    "print(sorted([l['linkCt'] for l in networkJson['links']],reverse=True)[:10])\n",
    "print(len(networkJson['links']))\n",
    "with open(f'../wordnet/data/{period}-wordnet2lines_allstops_2-21.json', 'w') as json_file:\n",
    "    json.dump(networkJson, json_file, ensure_ascii = False, indent=4)\n",
    "with open(f'../wordnet/data/{period}-label2lines_2-21.json', 'w') as json_file:\n",
    "    json.dump(label2lines, json_file, ensure_ascii=False, indent=4)\n",
    "print('wrote the jsons!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b63d9fe4-a59b-442c-80aa-b94b8b079d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'excerpt': 'Видимо когда что то привычное\\nменяет свою форму',\n",
       "  'author': 'Виктор Плавский',\n",
       "  'period': 'After',\n",
       "  'uniqueIndex': 172},\n",
       " {'excerpt': 'неразрывного одного объема\\nчуть меняющего форму когда в него бьют',\n",
       "  'author': 'Василий Бородин',\n",
       "  'period': 'Before',\n",
       "  'uniqueIndex': 2896}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2lines[a['sourceLemma']+'AND'+a['targetLemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceca441-2d9b-4f2b-a372-0f4f573a97a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
