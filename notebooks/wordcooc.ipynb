{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5c1f973-8e3c-42fb-beb2-8eae75e46db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d9e97f2-a55b-4a46-98ba-f6dcdfd86baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': 'я динозавр я прыгаю через кактусы\\nнекоторые кактусы выше меня ростом и стоят по трое\\nнельзя прыгать слишком рано\\nнужно выжидать и прыгать в самый последний момент\\nпотому что опасность бывает протяженной\\nопасность это угроза гибели\\nнужно бояться низко летящих птиц\\nиногда наступает ночь\\nиногда световой день становится короче\\nиногда птицы стараются меня перехитрить\\nя прыгаю с приятным звуком как игрушка\\nвсе это время бежит мой счетчик\\nиногда я бегу а противника все нет\\nя бегу совсем один\\nя умираю от любого соприкосновения с миром',\n",
       " 'Author': 'Александра Цибуля',\n",
       " 'Before or after': 'Before',\n",
       " 'Source': 'essentialpoetry',\n",
       " 'Date posted': datetime.datetime(2021, 5, 2, 0, 0),\n",
       " 'UniqueIndex': 2457}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../Excel_files/Full_Poem_Dataset_12-17.xlsx')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "records = df.to_dict('records')\n",
    "random.choice(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17506dec-bb0c-4b4b-85a0-07e7f46ba5e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Looking at most labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d02c1eee-e4f9-49ad-bd2a-8bf83a1986ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "79c98133-4834-41d5-a8ce-a51fe3516be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(networkJson['links'])\n",
    "keyword = 'язык'\n",
    "linkUps = {'Before' : [], 'After' : []}\n",
    "lst = []\n",
    "for link in networkJson['links']:\n",
    "    if link['sourceLemma'] == keyword or link['targetLemma'] == keyword:\n",
    "        # ct = link['linkCtBefore'] + link['linkCtBefore']\n",
    "        # if link['linkCtBefore'] > link['linkCtAfter']:\n",
    "        #     value = (((link['linkCtBefore'] + 1) - (link['linkCtAfter'] + 1)) / (link['linkCtAfter'] + 1))*ct\n",
    "        #     period = 'Before'\n",
    "        # else:\n",
    "        #     value = (((link['linkCtAfter'] + 1) - (link['linkCtBefore'] + 1)) / (link['linkCtBefore'] + 1))*ct\n",
    "        #     period = 'After'\n",
    "            \n",
    "        value = math.sqrt(link['linkCtAfter']) - math.sqrt(link['linkCtBefore'])\n",
    "        lst.append({'value' : value, 'info' : link})\n",
    "        # linkUps[period].append({'value' : value, 'info' : link})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b4a20372-7de1-42e8-aa92-5fa2f1560792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "молчать язык\n",
      "0 3\n",
      "прийти язык\n",
      "0 3\n",
      "слышать язык\n",
      "0 3\n",
      "ирпень язык\n",
      "0 2\n",
      "голубь язык\n",
      "0 2\n",
      "кровавый язык\n",
      "0 2\n",
      "граница язык\n",
      "0 2\n",
      "бродить язык\n",
      "0 2\n",
      "великий язык\n",
      "0 2\n",
      "страна язык\n",
      "2 8\n",
      "железный язык\n",
      "0 2\n",
      "шершавый язык\n",
      "0 2\n",
      "небесный язык\n",
      "0 2\n",
      "возвратиться язык\n",
      "0 2\n",
      "убить язык\n",
      "0 2\n",
      "взгляд язык\n",
      "0 2\n",
      "былой язык\n",
      "0 2\n",
      "менять язык\n",
      "0 2\n",
      "птичий язык\n",
      "0 2\n",
      "невозможный язык\n",
      "0 2\n"
     ]
    }
   ],
   "source": [
    "for d in sorted(lst, key=lambda x: x['value'],reverse=True)[:20]:\n",
    "    print(d['info']['sourceLemma'], d['info']['targetLemma']) \n",
    "    print(d['info']['linkCtBefore'],d['info']['linkCtAfter'])\n",
    "    # print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "13998a0e-a1d7-4684-bc41-77b8e6a336e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in linkUps['After']:\n",
    "#     if d['info']['linkCtBefore'] == 0 and d['info']['linkCtAfter'] > 2:\n",
    "#         print(d['info']['sourceLemma'], d['info']['targetLemma'], d['info']['linkCtBefore'],d['info']['linkCtAfter'])\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "dbb0876b-5c89-4f00-8d43-6d3eecd339b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(random.choice(networkJson['links']))\n",
    "# lst = []\n",
    "# for link in networkJson['links']:\n",
    "#     if link['sourceLemma'] == 'язык' or link['targetLemma'] == 'язык': \n",
    "#         lst.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "eb0402b2-e3cf-4960-ae60-0ad006cdfb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "русский язык\n",
      "6 12\n",
      "страна язык\n",
      "2 8\n",
      "земля язык\n",
      "2 6\n",
      "война язык\n",
      "1 5\n"
     ]
    }
   ],
   "source": [
    "for d in sorted(lst, key=lambda x: (x['linkCtBefore'] + x['linkCtAfter']),reverse=True):\n",
    "    # if d['linkCtBefore'] + d['linkCtAfter'] > 4:\n",
    "    if d['linkCtAfter'] - d['linkCtBefore'] > 3:\n",
    "        if d['linkCtBefore'] < d['linkCtAfter']:\n",
    "            # print(d['linkCtAfter'])\n",
    "            print(d['sourceLemma'], d['targetLemma']) \n",
    "            print(d['linkCtBefore'],d['linkCtAfter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7f20fbba-6190-4e93-b3b4-47748ed3d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in sorted(linkUps['After'], key=lambda x: x['value'],reverse=True)[]:\n",
    "#     print(d['info']['sourceLemma'], d['info']['targetLemma']) \n",
    "#     print(d['info']['linkCtBefore'],d['info']['linkCtAfter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26466c-1b62-4a16-9dd4-f516e5441565",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Checking specific labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a3b4acd5-8405-466b-b332-36b15a76ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../label2lines_3-3.json', 'r', encoding='utf-8') as f:\n",
    "    linedata = label2lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b079e3dd-8db6-46f6-b70e-585e25329774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'порождатьANDсуществовать'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(linedata.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "018e50ce-91af-4597-a44b-d83cfadaa860",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'западANDкиев'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jj/_szc94p56q91q_7c209d1d9w0000gn/T/ipykernel_61286/2338696909.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'киев'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'AND'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinedata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'period'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'excerpt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'западANDкиев'"
     ]
    }
   ],
   "source": [
    "w1 = 'запад'\n",
    "w2 = 'киев'\n",
    "key ='AND'.join(sorted([w1,w2]))\n",
    "for d in linedata[key]:\n",
    "    print(d['period'], '\\n', d['excerpt'],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac76d1-ab27-4756-9d0f-b1b62d7f3bc9",
   "metadata": {},
   "source": [
    "## nearest neighbors of cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9a713a43-8640-4dea-97aa-55e6e2745d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../files2big/before-after-average-BERT-PROPN_0.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e63db0b4-be2b-4987-8cd5-5d87f48d82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../files2big/before-after-average-BERT-1.json', 'r', encoding='utf-8') as f:\n",
    "    newdata = json.load(f)\n",
    "data.update(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "250538d0-229a-404c-83f5-d614e7bedca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in t.split('\\n'):\n",
    "    newdata[place] = data[place]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "152a1d70-ff09-4bd6-98de-c9cda780b89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851, 1047)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newdata), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c7063df-257a-4672-b906-42c85a0fa6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'../../files2big/all-BERT.json', 'w') as json_file:\n",
    "#     json.dump(newdata, json_file, ensure_ascii = False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "668a4d58-5d2c-45e6-aeb7-f1517d68eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../averageTSNE/pnouns.txt','r') as f:\n",
    "#     pns = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6c5b570-491b-4070-af07-58f9686ade3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'../averageTSNE/places.json', 'w') as json_file:\n",
    "#     json.dump(t.split('\\n'), json_file, ensure_ascii = False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9cec9d9-12bd-47cc-ae9e-09250250e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosim(a,b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bab2dffd-c329-4947-83ec-7aa229933df7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1047/1047 [00:00<00:00, 1704.07it/s]\n"
     ]
    }
   ],
   "source": [
    "keyword = 'крым'\n",
    "periods = ['Average','Before','After']\n",
    "\n",
    "cosinesims = dict()\n",
    "for d in tqdm(data):\n",
    "    for p in periods:\n",
    "        cosinesims.setdefault(p, [])\n",
    "        if not data[d][p]==np.zeros(768).tolist() and not data[keyword][p]==np.zeros(768).tolist():\n",
    "            cosinesims[p].append({'cosim' : cosim(data[d][p],data[keyword][p]), 'word' : d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fab075d2-2840-41d2-9d62-4166d70dcd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cosim': 1.0, 'word': 'крым'},\n",
       " {'cosim': 0.7425093397298371, 'word': 'россия'},\n",
       " {'cosim': 0.7065024042823413, 'word': 'рим'},\n",
       " {'cosim': 0.6955584494972565, 'word': 'киев'},\n",
       " {'cosim': 0.6812121104130849, 'word': 'граница'},\n",
       " {'cosim': 0.6794069488443558, 'word': 'армия'},\n",
       " {'cosim': 0.6775894950695878, 'word': 'кавказ'},\n",
       " {'cosim': 0.671969498752865, 'word': 'глубина'},\n",
       " {'cosim': 0.6688129958740999, 'word': 'история'},\n",
       " {'cosim': 0.6675298867941951, 'word': 'ад'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cosinesims['Before'], key=lambda x: x['cosim'],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06c4c024-bb7c-45ea-95af-1a1810c89eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cosim': 1.0, 'word': 'крым'},\n",
       " {'cosim': 0.6922362931835234, 'word': 'россия'},\n",
       " {'cosim': 0.6765936834348142, 'word': 'киев'},\n",
       " {'cosim': 0.6550983838054637, 'word': 'днепр'},\n",
       " {'cosim': 0.6543305039288093, 'word': 'рим'},\n",
       " {'cosim': 0.637430673756912, 'word': 'чернигов'},\n",
       " {'cosim': 0.6285666554363327, 'word': 'армия'},\n",
       " {'cosim': 0.6271573085733786, 'word': 'роза'},\n",
       " {'cosim': 0.6235303197932859, 'word': 'марс'},\n",
       " {'cosim': 0.6203796455746422, 'word': 'граница'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cosinesims['After'], key=lambda x: x['cosim'],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "44c7c495-a909-4162-87d2-8eb088fd576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['бумага', 'разговор', 'зуб', 'поэт', 'народ', 'граница', 'сложный', 'песня', 'общий', 'страна', 'берег', 'сторона', 'корень', 'знак', 'звук', 'рука', 'ребёнок', 'предмет', 'лодка', 'слово', 'крик', 'говорить', 'речь', 'губа', 'корабль', 'враг', 'книга', 'стих', 'кожа', 'кость', 'лист', 'земля', 'русский', 'голос', 'взгляд', 'язык', 'дух']\n"
     ]
    }
   ],
   "source": [
    "ws = set([a['word'].split('_')[0] for a in sorted(aftcosinesims, key=lambda x: x['cosim'],reverse=True)[:30] + sorted(befcosinesims, key=lambda x: x['cosim'],reverse=True)[:30]])\n",
    "print(list(ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b9229-bc58-4f3e-a0ae-28a37789f49f",
   "metadata": {},
   "source": [
    "## Word Co-occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b227b56-aeef-4944-aff5-1235b96ad4ec",
   "metadata": {},
   "source": [
    "### Initialize Spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "371f9312-6a74-41ee-aa5e-21f121b3be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cc2dcde1-6e44-45a8-9608-4fab8aeafbcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download uk_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52702e34-a4ff-4035-a28a-2d0c84216d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download ru_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b3b5557-9ce4-4537-8499-a02eda62f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_lg\", disable=['attribute_ruler','ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942516f5-328e-43c1-bbe7-7569aaafc87f",
   "metadata": {},
   "source": [
    "### Generate Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b1efa21f-a33d-4adb-9c00-b8f280d09221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3222/3222 [03:07<00:00, 17.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for rec in tqdm(records):\n",
    "    recTxt = rec['Text']\n",
    "    # if not isinstance(recTxt,str):\n",
    "    #     recTxt = str(recTxt)\n",
    "    doc = nlp(recTxt)\n",
    "    rec['spacydoc'] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b179b-158c-4d4e-a97b-897dee35c264",
   "metadata": {},
   "source": [
    "### Run cooc parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "05b78a34-0444-4d3d-93a0-9a06e49006b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "140544ad-c44a-4e92-81e1-0da4761dfc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.ru import stop_words\n",
    "ru_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.en import stop_words\n",
    "en_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.uk import stop_words\n",
    "uk_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.es import stop_words\n",
    "es_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.pl import stop_words\n",
    "pl_stops = stop_words.STOP_WORDS\n",
    "\n",
    "stopwords = list(ru_stops)+list(uk_stops)\n",
    "foreign_stops = list(en_stops)+list(es_stops)+list(pl_stops)\n",
    "short_stops = [s for s in stopwords if len(s) < 3]\n",
    "short_ru_stops = [s for s in ru_stops if len(s) < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e983f2f6-8b37-4ea4-8973-2a4965df02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isLemma(txt):\n",
    "    for p in string.punctuation+'—'+'–':\n",
    "        if p in txt:\n",
    "            return False\n",
    "    if txt.isspace():\n",
    "        return False\n",
    "    if txt in foreign_stops:\n",
    "        return False\n",
    "    # if txt in short_stops:\n",
    "    #     return False\n",
    "    if txt in stopwords:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d8995008-9391-4d6c-96cd-28eee8c64428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isLemma('говорить')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3c46ccf2-92dd-48a4-a059-550c1b6002fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isLemma('сказать')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202d39e-5e2f-44de-95d0-f0e0811d1b62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Before and after separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed635a2f-dc8d-4b57-b0d5-2fdea3c2ca6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding sufficient nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3222/3222 [00:02<00:00, 1281.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3222/3222 [00:11<00:00, 291.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 33, 27, 26, 25, 24, 23, 23, 22, 22]\n",
      "162859\n",
      "wrote the jsons!\n"
     ]
    }
   ],
   "source": [
    "for period in ['Before','After'][1:]:\n",
    "    \n",
    "    lens=set()\n",
    "    lenCts = dict()\n",
    "\n",
    "    networkJson = dict()\n",
    "    networkJson['nodes'] = []\n",
    "    networkJson['links'] = []\n",
    "\n",
    "    curatedNodes = set()\n",
    "    linkCounter = dict()\n",
    "    lemmaCounter = dict()\n",
    "\n",
    "    print('finding sufficient nodes...')\n",
    "\n",
    "    for rec in tqdm(records):\n",
    "        # if rec['Before or after'] != period:\n",
    "            # continue\n",
    "            \n",
    "        recTxt = rec['Text']\n",
    "\n",
    "        doc = rec['spacydoc']\n",
    "        for t1 in doc:\n",
    "            l1 = t1.lemma_.lower()\n",
    "            lemmaCounter.setdefault(l1, 0)\n",
    "            lemmaCounter[l1] += 1\n",
    "\n",
    "    # find those nodes which occur enough times\n",
    "    sufficientNodes = set()\n",
    "    for lemma in lemmaCounter:\n",
    "        if lemmaCounter[lemma] >= 5 and isLemma(lemma):\n",
    "            sufficientNodes.add(lemma.lower())\n",
    "\n",
    "\n",
    "    label2lines = dict()\n",
    "    # label2recs = dict()\n",
    "    label2authors = dict()\n",
    "\n",
    "    print('parsing links...')\n",
    "    for rec in tqdm(records):\n",
    "        \n",
    "        if rec['Before or after'] != period:\n",
    "            continue\n",
    "        \n",
    "        doc = rec['spacydoc']\n",
    "        newdoc = [t for t in doc if (t.text.isalpha() or t.text == '\\n')]\n",
    "        doclen = len(newdoc)\n",
    "        # # line jumping\n",
    "        allLines = []\n",
    "        newLine = []\n",
    "        for i, t in enumerate(newdoc):\n",
    "            if t.text == '\\n':\n",
    "                allLines.append(newLine)\n",
    "                newLine = []\n",
    "            elif i == doclen-1:\n",
    "                allLines.append(newLine)\n",
    "            else:\n",
    "                newLine.append(t)\n",
    "        numLines = len(allLines)\n",
    "        windowlength = 2\n",
    "\n",
    "        labelFoundFromLine = dict()\n",
    "        for i1 in range(numLines-windowlength):\n",
    "            # all tokens in the desired range\n",
    "            tokensInWindow = []\n",
    "            excerptLines = []\n",
    "            for line in allLines[i1:i1+windowlength]:\n",
    "                excerptLines.append(' '.join([t.text for t in line]))\n",
    "                for token in line:\n",
    "                    tokensInWindow.append(token)\n",
    "            tiw = len(tokensInWindow)\n",
    "            lens.add(tiw)\n",
    "            lenCts.setdefault(tiw,0)\n",
    "            lenCts[tiw] += 1\n",
    "            if tiw > 50:\n",
    "                continue\n",
    "\n",
    "            for ic, (t1, t2) in enumerate(itertools.combinations(tokensInWindow,2)):\n",
    "                l1 = t1.lemma_.lower()\n",
    "                l2 = t2.lemma_.lower()\n",
    "                if l1 != l2 and l1 in sufficientNodes and l2 in sufficientNodes:\n",
    "                    label = 'AND'.join(sorted([l1, l2]))\n",
    "\n",
    "                    ### label already found in line?\n",
    "                    repeatedInstance = False\n",
    "                    for line in excerptLines:\n",
    "                        labelFoundFromLine.setdefault((label, line), False)\n",
    "                        if labelFoundFromLine[(label, line)]:\n",
    "                            repeatedInstance = True\n",
    "                        else:\n",
    "                            labelFoundFromLine[(label, line)] = True\n",
    "                    if repeatedInstance:\n",
    "                        continue\n",
    "\n",
    "                    label2authors.setdefault(label, set())\n",
    "                    label2authors[label].add(rec['Author'])\n",
    "#                     label2recs.setdefault(label, [])\n",
    "#                     if rec not in label2recs[label]:\n",
    "#                         label2recs[label].append(rec)\n",
    "\n",
    "                    excerpt = '\\n'.join(excerptLines)\n",
    "                    label2lines.setdefault(label, [])\n",
    "                    if excerpt not in [e['excerpt'] for e in label2lines[label]]:\n",
    "                        # increase link Ct\n",
    "                        linkCounter.setdefault(label, 0)\n",
    "                        linkCounter[label] += 1\n",
    "                        period = rec['Before or after']\n",
    "                        if pd.isna(period):\n",
    "                            period = ''\n",
    "                        author = rec['Author']\n",
    "                        if pd.isna(author):\n",
    "                            author = 'Unknown'\n",
    "                        label2lines[label].append({\n",
    "                            'excerpt' : excerpt,\n",
    "                            'author' : author,\n",
    "                            'period' : period,\n",
    "                            'uniqueIndex' : rec['UniqueIndex']\n",
    "                        })\n",
    "\n",
    "    node2id = dict()\n",
    "    for i, node in enumerate(sufficientNodes):\n",
    "        networkJson['nodes'].append({\n",
    "            'id' : node,\n",
    "            'totalinstances' : lemmaCounter[node]\n",
    "        })\n",
    "        node2id[node] = i\n",
    "\n",
    "    for label in linkCounter:\n",
    "        source = label.split('AND')[0]\n",
    "        target = label.split('AND')[1]\n",
    "        if lemmaCounter[source] > 9 and lemmaCounter[target] > 9:\n",
    "            networkJson['links'].append({\n",
    "                'source' : node2id[source],\n",
    "                'target' : node2id[target],\n",
    "                'sourceLemma' : source,\n",
    "                'targetLemma' : target,\n",
    "                'linkCt' : linkCounter[label],\n",
    "                'authorCt' : list(label2authors[label])\n",
    "            })\n",
    "    print(sorted([l['linkCt'] for l in networkJson['links']],reverse=True)[:10])\n",
    "    print(len(networkJson['links']))\n",
    "    with open(f'../wordnet/data/{period}-wordnet2lines_allstops_2-21.json', 'w') as json_file:\n",
    "        json.dump(networkJson, json_file, ensure_ascii = False, indent=4)\n",
    "    with open(f'../wordnet/data/{period}-label2lines_2-21.json', 'w') as json_file:\n",
    "        json.dump(label2lines, json_file, ensure_ascii=False, indent=4)\n",
    "    print('wrote the jsons!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7eea5d-5a9a-4791-9cbd-1f0341b0fa4f",
   "metadata": {},
   "source": [
    "## Before and After Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dfc7f9c6-1b6a-446f-9e0d-683086b20ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding sufficient nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3222/3222 [00:01<00:00, 2294.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3222/3222 [00:17<00:00, 182.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 33, 27, 26, 25, 24, 23, 23, 22, 22]\n",
      "310725\n",
      "wrote the json!\n"
     ]
    }
   ],
   "source": [
    "lens=set()\n",
    "lenCts = dict()\n",
    "\n",
    "networkJson = dict()\n",
    "networkJson['nodes'] = []\n",
    "networkJson['links'] = []\n",
    "\n",
    "curatedNodes = set()\n",
    "linkCounter = {'Before' : dict(), 'After' : dict()}\n",
    "lemmaCounter = dict()\n",
    "\n",
    "print('finding sufficient nodes...')\n",
    "\n",
    "for rec in tqdm(records):\n",
    "    # if rec['Before or after'] != period:\n",
    "        # continue\n",
    "\n",
    "    recTxt = rec['Text']\n",
    "\n",
    "    doc = rec['spacydoc']\n",
    "    for t1 in doc:\n",
    "        l1 = t1.lemma_.lower()\n",
    "        lemmaCounter.setdefault(l1, 0)\n",
    "        lemmaCounter[l1] += 1\n",
    "\n",
    "# find those nodes which occur enough times\n",
    "sufficientNodes = set()\n",
    "for lemma in lemmaCounter:\n",
    "    if lemmaCounter[lemma] >= 5 and isLemma(lemma):\n",
    "        sufficientNodes.add(lemma.lower())\n",
    "\n",
    "\n",
    "label2lines = dict()\n",
    "# label2recs = dict()\n",
    "label2authors = dict()\n",
    "allLabels = set()\n",
    "\n",
    "print('parsing links...')\n",
    "for rec in tqdm(records):\n",
    "\n",
    "    period = rec['Before or after']\n",
    "\n",
    "    doc = rec['spacydoc']\n",
    "    newdoc = [t for t in doc if (t.text.isalpha() or t.text == '\\n')]\n",
    "    doclen = len(newdoc)\n",
    "    # # line jumping\n",
    "    allLines = []\n",
    "    newLine = []\n",
    "    for i, t in enumerate(newdoc):\n",
    "        if t.text == '\\n':\n",
    "            allLines.append(newLine)\n",
    "            newLine = []\n",
    "        elif i == doclen-1:\n",
    "            allLines.append(newLine)\n",
    "        else:\n",
    "            newLine.append(t)\n",
    "    numLines = len(allLines)\n",
    "    windowlength = 2\n",
    "\n",
    "    labelFoundFromLine = dict()\n",
    "    for i1 in range(numLines-windowlength):\n",
    "        # all tokens in the desired range\n",
    "        tokensInWindow = []\n",
    "        excerptLines = []\n",
    "        for line in allLines[i1:i1+windowlength]:\n",
    "            excerptLines.append(' '.join([t.text for t in line]))\n",
    "            for token in line:\n",
    "                tokensInWindow.append(token)\n",
    "        tiw = len(tokensInWindow)\n",
    "        lens.add(tiw)\n",
    "        lenCts.setdefault(tiw,0)\n",
    "        lenCts[tiw] += 1\n",
    "        if tiw > 50:\n",
    "            continue\n",
    "\n",
    "        for ic, (t1, t2) in enumerate(itertools.combinations(tokensInWindow,2)):\n",
    "            l1 = t1.lemma_.lower()\n",
    "            l2 = t2.lemma_.lower()\n",
    "            if l1 != l2 and l1 in sufficientNodes and l2 in sufficientNodes:\n",
    "                label = 'AND'.join(sorted([l1, l2]))\n",
    "\n",
    "                ### label already found in line?\n",
    "                repeatedInstance = False\n",
    "                for line in excerptLines:\n",
    "                    labelFoundFromLine.setdefault((label, line), False)\n",
    "                    if labelFoundFromLine[(label, line)]:\n",
    "                        repeatedInstance = True\n",
    "                    else:\n",
    "                        labelFoundFromLine[(label, line)] = True\n",
    "                if repeatedInstance:\n",
    "                    continue\n",
    "\n",
    "                label2authors.setdefault(label, {'Before' : set(), 'After' : set()})\n",
    "                label2authors[label][period].add(rec['Author'])\n",
    "\n",
    "\n",
    "                # label2recs.setdefault(label, [])\n",
    "                # if rec not in label2recs[label]:\n",
    "                #     label2recs[label].append(rec)\n",
    "\n",
    "                excerpt = '\\n'.join(excerptLines)\n",
    "                label2lines.setdefault(label, [])\n",
    "                if excerpt not in [e['excerpt'] for e in label2lines[label]]:\n",
    "                    # increase link Ct\n",
    "                    for p in ['Before','After']:\n",
    "                        linkCounter[p].setdefault(label, 0)\n",
    "                    linkCounter[period][label] += 1\n",
    "                    allLabels.add(label)\n",
    "\n",
    "                    if pd.isna(period):\n",
    "                        period = ''\n",
    "                    author = rec['Author']\n",
    "                    if pd.isna(author):\n",
    "                        author = 'Unknown'\n",
    "                    label2lines[label].append({\n",
    "                        'excerpt' : excerpt,\n",
    "                        'author' : author,\n",
    "                        'period' : period,\n",
    "                        'uniqueIndex' : rec['UniqueIndex']\n",
    "                    })\n",
    "\n",
    "node2id = dict()\n",
    "for i, node in enumerate(sufficientNodes):\n",
    "    networkJson['nodes'].append({\n",
    "        'id' : node,\n",
    "        'totalinstances' : lemmaCounter[node],\n",
    "        'pos' : nlp(node)[0].pos_\n",
    "    })\n",
    "    node2id[node] = i\n",
    "\n",
    "for label in allLabels:\n",
    "    source = label.split('AND')[0]\n",
    "    target = label.split('AND')[1]\n",
    "    if lemmaCounter[source] > 9 and lemmaCounter[target] > 9:\n",
    "        label2authors[label]['Before'] = list(label2authors[label]['Before'])\n",
    "        label2authors[label]['After'] = list(label2authors[label]['After'])\n",
    "        \n",
    "        networkJson['links'].append({\n",
    "            'source' : node2id[source],\n",
    "            'target' : node2id[target],\n",
    "            'sourceLemma' : source,\n",
    "            'targetLemma' : target,\n",
    "            'linkCtBefore' : linkCounter['Before'][label],\n",
    "            'linkCtAfter' : linkCounter['After'][label],\n",
    "            'authors' : label2authors[label],\n",
    "        })\n",
    "print(sorted([l['linkCtAfter'] for l in networkJson['links']],reverse=True)[:10])\n",
    "print(len(networkJson['links']))\n",
    "# with open(f'../../allLinkData3-3_3.json', 'w') as json_file:\n",
    "#     json.dump(networkJson, json_file, ensure_ascii = False, indent=4)\n",
    "# with open(f'../../label2lines_3-3.json', 'w') as json_file:\n",
    "#     json.dump(label2lines, json_file, ensure_ascii=False, indent=4)\n",
    "print('wrote the json!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b63d9fe4-a59b-442c-80aa-b94b8b079d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'excerpt': 'Видимо когда что то привычное\\nменяет свою форму',\n",
       "  'author': 'Виктор Плавский',\n",
       "  'period': 'After',\n",
       "  'uniqueIndex': 172},\n",
       " {'excerpt': 'неразрывного одного объема\\nчуть меняющего форму когда в него бьют',\n",
       "  'author': 'Василий Бородин',\n",
       "  'period': 'Before',\n",
       "  'uniqueIndex': 2896}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2lines[a['sourceLemma']+'AND'+a['targetLemma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377af3f-b5fe-47aa-b41e-81ce5887bc29",
   "metadata": {},
   "source": [
    "### Connections, translations, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cceca441-2d9b-4f2b-a372-0f4f573a97a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  occur the most with the propns\n",
    "counter = dict()\n",
    "for pn in pns.split('\\n'):\n",
    "    for link in networkJson['links']:\n",
    "        if pn == link['sourceLemma']:\n",
    "            targetWord = link['targetLemma']\n",
    "        elif pn == link['targetLemma']:\n",
    "            targetWord = link['sourceLemma']\n",
    "        else:\n",
    "            targetWord = ''\n",
    "        if targetWord != '':\n",
    "            counter.setdefault(targetWord, 0)\n",
    "            counter[targetWord] += link['linkCtAfter']\n",
    "lst = []\n",
    "for c in counter:\n",
    "    lst.append({'ct' : counter[c], 'word' : c})\n",
    "srted = sorted(lst, key=lambda x: x['ct'],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d225456f-8e40-47d5-a0e3-435326a58ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../averageTSNE/sortedConnections.json', 'w') as json_file:\n",
    "    json.dump(srted, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2baeb160-897d-49d6-8bc3-2b3da16ef9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using United States server backend.\n"
     ]
    }
   ],
   "source": [
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "42676a30-d695-46a8-9fd2-bff219f3cca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bucha'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.google('буча')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b38d995b-55b2-40ed-be64-d1b5fe61afdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 851/851 [20:28<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "translations = dict()\n",
    "for k in tqdm(newdata):\n",
    "    translations[k] = ts.google(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1342a53e-1907-4724-99ed-03bfb89b6418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851, 851)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newdata), len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ff9acc35-7a1f-4de5-b9cd-66be6e4a49a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kherson'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations['херсон']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3c30de51-ff2a-44df-8790-7a29e27ddbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'../averageTSNE/translationsBERT.json', 'w') as json_file:\n",
    "#     json.dump(translations, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a1dae1a7-0624-48bc-b4eb-37f9a3cd9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'../averageTSNE/lemmaCounter.json', 'w') as json_file:\n",
    "#     json.dump(lemmaCounter, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e2379250-8c15-41b7-a9ed-bc3e54be85b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmaCounter['египет']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b67ea-757d-4ec0-9522-bfc1159df132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
