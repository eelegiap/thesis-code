{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c1f973-8e3c-42fb-beb2-8eae75e46db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9e97f2-a55b-4a46-98ba-f6dcdfd86baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': 'москвичи по газону не ходят\\nмосквичи под газоном лежат\\nо копающий ямочку котик\\nдай оттудова лапу пожать\\nесли надо копать то конечно же\\nвзяв лопату копай молчи\\nв этой почве есть твёрдые нежные\\nкирпичи\\nчерепа\\nмосквичи',\n",
       " 'Author': 'Андрей Чемоданов',\n",
       " 'Before or after': 'Before',\n",
       " 'Source': 'essentialpoetry',\n",
       " 'Date posted': datetime.datetime(2019, 11, 1, 0, 0),\n",
       " 'UniqueIndex': 2800}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../Excel_files/Full_Poem_Dataset_12-17.xlsx')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "records = df.to_dict('records')\n",
    "random.choice(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b9229-bc58-4f3e-a0ae-28a37789f49f",
   "metadata": {},
   "source": [
    "## Word Co-occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b227b56-aeef-4944-aff5-1235b96ad4ec",
   "metadata": {},
   "source": [
    "### Initialize Spacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371f9312-6a74-41ee-aa5e-21f121b3be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2dcde1-6e44-45a8-9608-4fab8aeafbcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download uk_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52702e34-a4ff-4035-a28a-2d0c84216d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download ru_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3b5557-9ce4-4537-8499-a02eda62f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_lg\", disable=['attribute_ruler','ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942516f5-328e-43c1-bbe7-7569aaafc87f",
   "metadata": {},
   "source": [
    "### Generate Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1efa21f-a33d-4adb-9c00-b8f280d09221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3289/3289 [02:44<00:00, 19.98it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for rec in tqdm(records):\n",
    "    recTxt = rec['Text']\n",
    "    if not isinstance(recTxt,str):\n",
    "        recTxt = str(recTxt)\n",
    "    doc = nlp(recTxt)\n",
    "    rec['spacydoc'] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b179b-158c-4d4e-a97b-897dee35c264",
   "metadata": {},
   "source": [
    "### Run cooc parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b78a34-0444-4d3d-93a0-9a06e49006b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "140544ad-c44a-4e92-81e1-0da4761dfc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.ru import stop_words\n",
    "ru_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.en import stop_words\n",
    "en_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.uk import stop_words\n",
    "uk_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.es import stop_words\n",
    "es_stops = stop_words.STOP_WORDS\n",
    "\n",
    "from spacy.lang.pl import stop_words\n",
    "pl_stops = stop_words.STOP_WORDS\n",
    "\n",
    "stopwords = list(ru_stops)+list(uk_stops)\n",
    "foreign_stops = list(en_stops)+list(es_stops)+list(pl_stops)\n",
    "short_stops = [s for s in stopwords if len(s) < 3]\n",
    "short_ru_stops = [s for s in ru_stops if len(s) < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e983f2f6-8b37-4ea4-8973-2a4965df02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isLemma(txt):\n",
    "    for p in string.punctuation+'—'+'–':\n",
    "        if p in txt:\n",
    "            return False\n",
    "    if txt.isspace():\n",
    "        return False\n",
    "    if txt in foreign_stops:\n",
    "        return False\n",
    "    # if txt in short_stops:\n",
    "    #     return False\n",
    "    if txt in stopwords:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8995008-9391-4d6c-96cd-28eee8c64428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isLemma('говорить')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed635a2f-dc8d-4b57-b0d5-2fdea3c2ca6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding sufficient nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3289/3289 [00:00<00:00, 3388.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3289/3289 [00:12<00:00, 253.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 51, 47, 44, 43, 41, 40, 40, 40, 34]\n"
     ]
    }
   ],
   "source": [
    "lens=set()\n",
    "lenCts = dict()\n",
    "\n",
    "networkJson = dict()\n",
    "networkJson['nodes'] = []\n",
    "networkJson['links'] = []\n",
    "\n",
    "curatedNodes = set()\n",
    "linkCounter = dict()\n",
    "lemmaCounter = dict()\n",
    "\n",
    "print('finding sufficient nodes...')\n",
    "\n",
    "for rec in tqdm(records):\n",
    "    recTxt = rec['Text']\n",
    "\n",
    "    doc = rec['spacydoc']\n",
    "    for t1 in doc:\n",
    "        l1 = t1.lemma_.lower()\n",
    "        lemmaCounter.setdefault(l1, 0)\n",
    "        lemmaCounter[l1] += 1\n",
    "\n",
    "# find those nodes which occur enough times\n",
    "sufficientNodes = set()\n",
    "for lemma in lemmaCounter:\n",
    "    if lemmaCounter[lemma] >= 5 and isLemma(lemma):\n",
    "        sufficientNodes.add(lemma.lower())\n",
    "\n",
    "\n",
    "label2lines = dict()\n",
    "label2recs = dict()\n",
    "label2authors = dict()\n",
    "\n",
    "print('parsing links...')\n",
    "for rec in tqdm(records):\n",
    "    doc = rec['spacydoc']\n",
    "    newdoc = [t for t in doc if (t.text.isalpha() or t.text == '\\n')]\n",
    "    doclen = len(newdoc)\n",
    "    # # line jumping\n",
    "    allLines = []\n",
    "    newLine = []\n",
    "    for i, t in enumerate(newdoc):\n",
    "        if t.text == '\\n':\n",
    "            allLines.append(newLine)\n",
    "            newLine = []\n",
    "        elif i == doclen-1:\n",
    "            allLines.append(newLine)\n",
    "        else:\n",
    "            newLine.append(t)\n",
    "    numLines = len(allLines)\n",
    "    windowlength = 2\n",
    "\n",
    "    labelFoundFromLine = dict()\n",
    "    for i1 in range(numLines-windowlength):\n",
    "        # all tokens in the desired range\n",
    "        tokensInWindow = []\n",
    "        excerptLines = []\n",
    "        for line in allLines[i1:i1+windowlength]:\n",
    "            excerptLines.append(' '.join([t.text for t in line]))\n",
    "            for token in line:\n",
    "                tokensInWindow.append(token)\n",
    "        tiw = len(tokensInWindow)\n",
    "        lens.add(tiw)\n",
    "        lenCts.setdefault(tiw,0)\n",
    "        lenCts[tiw] += 1\n",
    "        if tiw > 50:\n",
    "            continue\n",
    "\n",
    "        for ic, (t1, t2) in enumerate(itertools.combinations(tokensInWindow,2)):\n",
    "            l1 = t1.lemma_.lower()\n",
    "            l2 = t2.lemma_.lower()\n",
    "            if l1 != l2 and l1 in sufficientNodes and l2 in sufficientNodes:\n",
    "                label = 'AND'.join(sorted([l1, l2]))\n",
    "\n",
    "                ### label already found in line?\n",
    "                repeatedInstance = False\n",
    "                for line in excerptLines:\n",
    "                    labelFoundFromLine.setdefault((label, line), False)\n",
    "                    if labelFoundFromLine[(label, line)]:\n",
    "                        repeatedInstance = True\n",
    "                    else:\n",
    "                        labelFoundFromLine[(label, line)] = True\n",
    "                if repeatedInstance:\n",
    "                    continue\n",
    "\n",
    "                label2authors.setdefault(label, set())\n",
    "                label2authors[label].add(rec['Author'])\n",
    "\n",
    "                label2recs.setdefault(label, [])\n",
    "                if rec not in label2recs[label]:\n",
    "                    label2recs[label].append(rec)\n",
    "\n",
    "                excerpt = '\\n'.join(excerptLines)\n",
    "                label2lines.setdefault(label, [])\n",
    "                if excerpt not in [e['excerpt'] for e in label2lines[label]]:\n",
    "                    # increase link Ct\n",
    "                    linkCounter.setdefault(label, 0)\n",
    "                    linkCounter[label] += 1\n",
    "                    period = rec['Before or after']\n",
    "                    if pd.isna(period):\n",
    "                        period = ''\n",
    "                    author = rec['Author']\n",
    "                    if pd.isna(author):\n",
    "                        author = 'Unknown'\n",
    "                    label2lines[label].append({\n",
    "                        'excerpt' : excerpt,\n",
    "                        'author' : author,\n",
    "                        'period' : period,\n",
    "                        'uniqueIndex' : rec['UniqueIndex']\n",
    "                    })\n",
    "\n",
    "node2id = dict()\n",
    "for i, node in enumerate(sufficientNodes):\n",
    "    networkJson['nodes'].append({\n",
    "        'id' : node,\n",
    "        'totalinstances' : lemmaCounter[node]\n",
    "    })\n",
    "    node2id[node] = i\n",
    "\n",
    "for label in linkCounter:\n",
    "    source = label.split('AND')[0]\n",
    "    target = label.split('AND')[1]\n",
    "    if linkCounter[label] > 4:\n",
    "        networkJson['links'].append({\n",
    "            'source' : node2id[source],\n",
    "            'target' : node2id[target],\n",
    "            'sourceLemma' : source,\n",
    "            'targetLemma' : target,\n",
    "            'linkCt' : linkCounter[label],\n",
    "            'authorCt' : list(label2authors[label])\n",
    "        })\n",
    "print(sorted([l['linkCt'] for l in networkJson['links']],reverse=True)[:10])\n",
    "with open(f'wordnet_2_lines_all_stops_21.json', 'w') as json_file:\n",
    "    json.dump(networkJson, json_file, ensure_ascii = False, indent=4)\n",
    "with open(f'label2lines_21.json', 'w') as json_file:\n",
    "    json.dump(label2lines, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c97d3d3-5cd2-4512-b84c-b00424259161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.4519522557151525"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sorted([l[''] for l in networkJson['links']],reverse=True))/len(networkJson['links'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "457ec08b-1aea-4711-95a2-0a2b4cb8a57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4943"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(networkJson['links'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfc7f9c6-1b6a-446f-9e0d-683086b20ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 4502, 'target': 1881, 'sourceLemma': 'менять', 'targetLemma': 'форма', 'linkCt': 2, 'authorCt': ['Виктор Плавский', 'Василий Бородин']}\n",
      "\n",
      "Видимо когда что то привычное\n",
      "меняет свою форму\n",
      "\n",
      "неразрывного одного объема\n",
      "чуть меняющего форму когда в него бьют\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = random.choice([l for l in networkJson['links'] if l['linkCt'] > 1])\n",
    "print(a)\n",
    "print()\n",
    "for l in label2lines[a['sourceLemma']+'AND'+a['targetLemma']]:\n",
    "    print(l['excerpt'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b63d9fe4-a59b-442c-80aa-b94b8b079d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'excerpt': 'Видимо когда что то привычное\\nменяет свою форму',\n",
       "  'author': 'Виктор Плавский',\n",
       "  'period': 'After',\n",
       "  'uniqueIndex': 172},\n",
       " {'excerpt': 'неразрывного одного объема\\nчуть меняющего форму когда в него бьют',\n",
       "  'author': 'Василий Бородин',\n",
       "  'period': 'Before',\n",
       "  'uniqueIndex': 2896}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2lines[a['sourceLemma']+'AND'+a['targetLemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceca441-2d9b-4f2b-a372-0f4f573a97a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
